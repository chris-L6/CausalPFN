{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a098247e",
   "metadata": {},
   "source": [
    "## Run the following setup code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0a72fc",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_patterns = \"*\"  # all of the datasets that are being run\n",
    "method_patterns = \"*\"  # all of the methods to run\n",
    "index = None\n",
    "replace_runs = False  # whether to replace existing runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb41ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int(index) if index is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718bed5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3b8d718470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from utils import *\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "load_dotenv(override=True)\n",
    "\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_patterns = dataset_patterns.split(\",\")\n",
    "method_patterns = method_patterns.split(\",\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 82718\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a35d5",
   "metadata": {},
   "source": [
    "Load all the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47de04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and required functions\n",
    "%autoreload 2\n",
    "from benchmarks import IHDPDataset, ACIC2016Dataset\n",
    "from benchmarks import RealCauseLalondeCPSDataset, RealCauseLalondePSIDDataset\n",
    "\n",
    "# Store all the results for all the datasets\n",
    "\n",
    "IHDP = IHDPDataset()\n",
    "ACIC2016 = ACIC2016Dataset()\n",
    "RealCauseLalondeCPS = RealCauseLalondeCPSDataset()\n",
    "RealCauseLalondePSID = RealCauseLalondePSIDDataset()\n",
    "\n",
    "datasets = {\n",
    "    \"IHDP\": IHDP if index is None else [IHDP[index%len(IHDP)]],\n",
    "    \"ACIC 2016\": ACIC2016 if index is None else [ACIC2016[index%len(ACIC2016)]],\n",
    "    \"RealCause Lalonde CPS\": RealCauseLalondeCPS if index is None else [RealCauseLalondeCPS[index%len(RealCauseLalondeCPS)]],\n",
    "    \"RealCause Lalonde PSID\": RealCauseLalondePSID if index is None else [RealCauseLalondePSID[index%len(RealCauseLalondePSID)]],\n",
    "}\n",
    "\n",
    "causal_effect_path = os.path.join(os.environ[\"OUTPUT_DIR\"], \"causal_effect/\")\n",
    "os.makedirs(causal_effect_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb959b16",
   "metadata": {},
   "source": [
    "## CausalPFN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97a715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CausalPFN: 100%|██████████| 310/310 [00:00<00:00, 5435.97it/s, dataset=RealCause Lalonde PSID]\n"
     ]
    }
   ],
   "source": [
    "from src.causalpfn import CATEEstimator, calculate_pehe, ATEEstimator\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=sum([len(dataset) for dataset in datasets.values()]),\n",
    "    desc=\"CausalPFN\",\n",
    ")\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    pbar.set_postfix(dataset=dataset_name)\n",
    "    for i in range(len(dataset)):\n",
    "        # dataset_name: str, method_name: str, all_method_patterns: list, all_datasets_patterns: list, idx: int, artifact_dir: str, replace: bool = False\n",
    "        with result_saver(\n",
    "            dataset_name=dataset_name,\n",
    "            method_name=\"CausalPFN\",\n",
    "            all_method_patterns=method_patterns,\n",
    "            all_datasets_patterns=dataset_patterns,\n",
    "            idx=i if index is None else index,\n",
    "            artifact_dir=causal_effect_path,\n",
    "            replace=replace_runs,\n",
    "        ) as result:\n",
    "            if result is not None:\n",
    "                cate_dset, ate_dset = dataset[i]\n",
    "\n",
    "                # CATE\n",
    "                time_start = time.time()\n",
    "                cate_estimator = CATEEstimator(\n",
    "                    device=device,\n",
    "                )\n",
    "                cate_estimator.fit(cate_dset.X_train, cate_dset.t_train, cate_dset.y_train)\n",
    "                estimated_cate = cate_estimator.estimate_cate(X=cate_dset.X_test)\n",
    "                time_spent = time.time() - time_start\n",
    "                pehe = calculate_pehe(cate_pred=estimated_cate, cate_true=cate_dset.true_cate)\n",
    "                result[\"pehe\"] = pehe\n",
    "                result[\"time_cate\"] = time_spent / (len(cate_dset.X_test) + len(cate_dset.X_train)) * 1000\n",
    "\n",
    "                # ATE\n",
    "                time_start = time.time()\n",
    "                ate_estimator = ATEEstimator(\n",
    "                    device=device,\n",
    "                )\n",
    "                ate_estimator.fit(ate_dset.X, ate_dset.t, ate_dset.y)\n",
    "                estimated_ate = ate_estimator.estimate_ate()\n",
    "                time_spent = time.time() - time_start\n",
    "                result[\"ate_rel_err\"] = abs(estimated_ate - ate_dset.true_ate) / abs(ate_dset.true_ate)\n",
    "                result[\"time_ate\"] = time_spent / len(ate_dset.X) * 1000\n",
    "\n",
    "            pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11a696",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3423432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baselines: 100%|██████████| 8990/8990 [00:01<00:00, 6258.27it/s, baseline=IPW (HPO), dataset=RealCause Lalonde PSID]                 \n"
     ]
    }
   ],
   "source": [
    "# Baselines (Base)\n",
    "from benchmarks.baselines import BaselineModel\n",
    "\n",
    "# Baselines (EconML)\n",
    "from benchmarks.baselines import (\n",
    "    ForestDMLBaseline,\n",
    "    TLearnerBaseline,\n",
    "    SLearnerBaseline,\n",
    "    XLearnerBaseline,\n",
    "    DALearnerBaseline,\n",
    "    XLearnerBaseline,\n",
    "    ForestDRLearnerBaseline,\n",
    ")\n",
    "\n",
    "# Baselines (CATE Net)\n",
    "from benchmarks.baselines import (\n",
    "    TarNetBaseline,\n",
    "    DragonNetBaseline,\n",
    "    RANetBaseline,\n",
    "    SNetBaseline,\n",
    "    FlexTENetBaseline,\n",
    "    XNetBaseline,\n",
    ")\n",
    "\n",
    "# GRF & BART & IPW\n",
    "from benchmarks.baselines import GRFBaseline, BartBaseline, IPWBaseline\n",
    "\n",
    "\n",
    "baselines = {\n",
    "    \"T Learner (no HPO)\": TLearnerBaseline(hpo=False),\n",
    "    \"T Learner (HPO)\": TLearnerBaseline(hpo=True),\n",
    "    \"S Learner (no HPO)\": SLearnerBaseline(hpo=False),\n",
    "    \"S Learner (HPO)\": SLearnerBaseline(hpo=True),\n",
    "    \"X Learner (no HPO)\": XLearnerBaseline(hpo=False),\n",
    "    \"X Learner (HPO)\": XLearnerBaseline(hpo=True),\n",
    "    \"DA Learner (no HPO)\": DALearnerBaseline(hpo=False),\n",
    "    \"DA Learner (HPO)\": DALearnerBaseline(hpo=True),\n",
    "    \"Forest DR Learner (no HPO)\": ForestDRLearnerBaseline(hpo=False),\n",
    "    \"Forest DR Learner (HPO)\": ForestDRLearnerBaseline(hpo=True),\n",
    "    \"Forest DML (no HPO)\": ForestDMLBaseline(hpo=False),\n",
    "    \"Forest DML (HPO)\": ForestDMLBaseline(hpo=True),\n",
    "    \"DragonNet (no HPO)\": DragonNetBaseline(hpo=False),\n",
    "    \"DragonNet (HPO)\": DragonNetBaseline(hpo=True),\n",
    "    \"TarNet (no HPO)\": TarNetBaseline(hpo=False),\n",
    "    \"TarNet (HPO)\": TarNetBaseline(hpo=True),\n",
    "    \"SNet (no HPO)\": SNetBaseline(hpo=False),\n",
    "    \"SNet (HPO)\": SNetBaseline(hpo=True),\n",
    "    \"FlexTENet (no HPO)\": FlexTENetBaseline(hpo=False, batch_size=256),\n",
    "    \"FlexTENet (HPO)\": FlexTENetBaseline(hpo=True, batch_size=256),\n",
    "    \"XNET (no HPO)\": XNetBaseline(hpo=False, batch_size=256),\n",
    "    \"XNET (HPO)\": XNetBaseline(hpo=True, batch_size=256),\n",
    "    \"RA Net (no HPO)\": RANetBaseline(hpo=False),\n",
    "    \"RA Net (HPO)\": RANetBaseline(hpo=True),\n",
    "    \"GRF (no HPO)\": GRFBaseline(hpo=False),\n",
    "    \"GRF (HPO)\": GRFBaseline(hpo=True),\n",
    "    \"BART\": BartBaseline(hpo=False),\n",
    "    \"IPW (no HPO)\": IPWBaseline(hpo=False),\n",
    "    \"IPW (HPO)\": IPWBaseline(hpo=True),\n",
    "}\n",
    "\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=sum([len(dataset) * len(baselines) for dataset in datasets.values()]),\n",
    "    desc=\"Baselines\",\n",
    ")\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    for baseline_name, baseline in baselines.items():\n",
    "        pbar.set_postfix(dataset=dataset_name, baseline=baseline_name)\n",
    "        for i in range(len(dataset)):\n",
    "            with result_saver(\n",
    "                dataset_name=dataset_name,\n",
    "                method_name=baseline_name,\n",
    "                all_method_patterns=method_patterns,\n",
    "                all_datasets_patterns=dataset_patterns,\n",
    "                idx=i if index is None else index,\n",
    "                artifact_dir=causal_effect_path,\n",
    "                replace=replace_runs,\n",
    "            ) as result:\n",
    "                if result is not None:\n",
    "                    baseline: BaselineModel\n",
    "                    cate_dset, ate_dset = dataset[i]\n",
    "\n",
    "                    # CATE\n",
    "                    start_time = time.time()\n",
    "                    cate_pred = baseline.estimate_cate(\n",
    "                        X_train=cate_dset.X_train,\n",
    "                        t_train=cate_dset.t_train,\n",
    "                        y_train=cate_dset.y_train,\n",
    "                        X_test=cate_dset.X_test,\n",
    "                    )\n",
    "                    time_spent = time.time() - start_time\n",
    "                    pehe = calculate_pehe(cate_true=cate_dset.true_cate, cate_pred=cate_pred)\n",
    "                    result[\"pehe\"] = pehe\n",
    "                    result[\"time_cate\"] = time_spent / (len(cate_dset.X_test) + len(cate_dset.X_train)) * 1000\n",
    "\n",
    "                    # ATE\n",
    "                    start_time = time.time()\n",
    "                    ate_pred = baseline.estimate_ate(\n",
    "                        X=ate_dset.X,\n",
    "                        t=ate_dset.t,\n",
    "                        y=ate_dset.y,\n",
    "                    )\n",
    "                    time_spent = time.time() - start_time\n",
    "                    result[\"ate_rel_err\"] = abs(ate_pred - ate_dset.true_ate) / abs(ate_dset.true_ate)\n",
    "                    result[\"time_ate\"] = time_spent / len(ate_dset.X) * 1000\n",
    "                pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a99ed",
   "metadata": {},
   "source": [
    "## Parse and visualize all of the results\n",
    "\n",
    "Once done, parse all of the results that were stored with the following code into a dataframe. This dataframe will contain different rows for each causal task and columns for the dataset, fold, method, and metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa354b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "methods_to_show = [\"CausalPFN\"] + list(baselines.keys())\n",
    "methods_to_show = [method for method in methods_to_show if any([check_match(method, m) for m in method_patterns])]\n",
    "results_df = pd.DataFrame(columns=[\"dataset\", \"method\", \"pehe\", \"ate_rel_err\", \"time_cate\", \"time_ate\", \"realization\"])\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dset_result = load_all_results(dataset_name, causal_effect_path)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"pehe\"])\n",
    "        for fold_idx in range(num_realizations):\n",
    "            pehe = float(all_rows[\"pehe\"][fold_idx])\n",
    "            ate_rel_err = float(all_rows[\"ate_rel_err\"][fold_idx])\n",
    "            time_cate = float(all_rows[\"time_cate\"][fold_idx])\n",
    "            time_ate = float(all_rows[\"time_ate\"][fold_idx])\n",
    "            new_row = dict(\n",
    "                dataset=dataset_name,\n",
    "                method=method,\n",
    "                cate_pehe=pehe,\n",
    "                ate_rel_err=ate_rel_err,\n",
    "                cate_time=time_cate,\n",
    "                ate_time=time_ate,\n",
    "                realization=fold_idx,\n",
    "            )\n",
    "            results_df = pd.concat([results_df, pd.DataFrame(new_row, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137175e",
   "metadata": {},
   "source": [
    "Once done, you can load the results below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all of the ATE and CATE estimation times by averaging over realizations\n",
    "time_spent_df = (\n",
    "    results_df.pivot_table(\n",
    "        index=\"method\",  # rows: one per method\n",
    "        columns=\"dataset\",  # multi‐columns: first level will be dataset\n",
    "        values=[\"ate_time\", \"cate_time\"],  # the values to aggregate\n",
    "        aggfunc=\"mean\",  # take the mean over realizations\n",
    "    )\n",
    "    .swaplevel(0, 1, axis=1)\n",
    "    .sort_index(axis=1, level=0)\n",
    ")\n",
    "\n",
    "# Compute mean and standard error for ATE and CATE metrics\n",
    "metrics = [\"cate_pehe\", \"ate_rel_err\"]\n",
    "grp = results_df.groupby([\"method\", \"dataset\"])[metrics].agg([\"mean\", \"sem\"])  # MultiIndex cols: (metric, agg)\n",
    "methods = grp.index.levels[0]\n",
    "datasets_index = grp.index.levels[1]\n",
    "data = {}\n",
    "for ds in datasets_index:\n",
    "    for m in metrics:\n",
    "        means = grp[(m, \"mean\")].xs(ds, level=\"dataset\")\n",
    "        sems = grp[(m, \"sem\")].xs(ds, level=\"dataset\")\n",
    "        # combine into \"xx.xx ± yy.yy\" strings\n",
    "        data[(ds, m)] = means.combine(sems, lambda mu, se: f\"{mu:.2f} ± {se:.2f}\")\n",
    "causal_effect_errors = pd.DataFrame(data, index=methods)\n",
    "causal_effect_errors.columns = pd.MultiIndex.from_tuples(causal_effect_errors.columns, names=[\"dataset\", \"metric\"])\n",
    "causal_effect_errors = causal_effect_errors.sort_index(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401a1ce",
   "metadata": {},
   "source": [
    "Compute total performance for CATE (rank of PEHE), and ATE (average relative error) for each method and all causal tasks (across different realizations of each method). Then visualize all of the error rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763e04d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ACIC 2016</th>\n",
       "      <th colspan=\"2\" halign=\"left\">IHDP</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RealCause Lalonde CPS</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RealCause Lalonde PSID</th>\n",
       "      <th colspan=\"3\" halign=\"left\">overall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>ate_rel_err</th>\n",
       "      <th>cate_pehe</th>\n",
       "      <th>ate_rel_err</th>\n",
       "      <th>cate_pehe</th>\n",
       "      <th>ate_rel_err</th>\n",
       "      <th>cate_pehe</th>\n",
       "      <th>ate_rel_err</th>\n",
       "      <th>cate_pehe</th>\n",
       "      <th>cate_rank ± ste</th>\n",
       "      <th>ate_rank ± ste</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CausalPFN</th>\n",
       "      <td>0.05 ± 0.01</td>\n",
       "      <td>0.92 ± 0.11</td>\n",
       "      <td>0.20 ± 0.04</td>\n",
       "      <td>0.58 ± 0.07</td>\n",
       "      <td>0.13 ± 0.01</td>\n",
       "      <td>8955.82 ± 20.92</td>\n",
       "      <td>0.22 ± 0.02</td>\n",
       "      <td>14402.01 ± 198.32</td>\n",
       "      <td>3.85 ± 0.20</td>\n",
       "      <td>8.69 ± 0.41</td>\n",
       "      <td>6.272581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T Learner (HPO)</th>\n",
       "      <td>0.03 ± 0.01</td>\n",
       "      <td>0.76 ± 0.07</td>\n",
       "      <td>0.21 ± 0.04</td>\n",
       "      <td>1.73 ± 0.30</td>\n",
       "      <td>0.24 ± 0.02</td>\n",
       "      <td>9223.92 ± 36.35</td>\n",
       "      <td>0.16 ± 0.03</td>\n",
       "      <td>15162.79 ± 462.18</td>\n",
       "      <td>6.46 ± 0.34</td>\n",
       "      <td>8.45 ± 0.40</td>\n",
       "      <td>7.456452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA Learner (no HPO)</th>\n",
       "      <td>0.09 ± 0.03</td>\n",
       "      <td>1.88 ± 0.24</td>\n",
       "      <td>0.22 ± 0.04</td>\n",
       "      <td>2.98 ± 0.51</td>\n",
       "      <td>0.22 ± 0.01</td>\n",
       "      <td>9011.68 ± 23.47</td>\n",
       "      <td>0.08 ± 0.01</td>\n",
       "      <td>13959.67 ± 192.75</td>\n",
       "      <td>6.87 ± 0.42</td>\n",
       "      <td>8.11 ± 0.41</td>\n",
       "      <td>7.490323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA Learner (HPO)</th>\n",
       "      <td>0.03 ± 0.01</td>\n",
       "      <td>0.72 ± 0.08</td>\n",
       "      <td>0.23 ± 0.04</td>\n",
       "      <td>2.07 ± 0.36</td>\n",
       "      <td>0.27 ± 0.02</td>\n",
       "      <td>9387.08 ± 56.81</td>\n",
       "      <td>0.20 ± 0.03</td>\n",
       "      <td>14548.75 ± 235.63</td>\n",
       "      <td>6.60 ± 0.35</td>\n",
       "      <td>9.48 ± 0.41</td>\n",
       "      <td>8.040323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T Learner (no HPO)</th>\n",
       "      <td>0.11 ± 0.03</td>\n",
       "      <td>2.06 ± 0.20</td>\n",
       "      <td>0.22 ± 0.04</td>\n",
       "      <td>2.94 ± 0.49</td>\n",
       "      <td>0.40 ± 0.01</td>\n",
       "      <td>9291.93 ± 23.20</td>\n",
       "      <td>0.07 ± 0.01</td>\n",
       "      <td>13910.37 ± 176.85</td>\n",
       "      <td>7.52 ± 0.39</td>\n",
       "      <td>9.12 ± 0.39</td>\n",
       "      <td>8.319355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPW (HPO)</th>\n",
       "      <td>0.21 ± 0.05</td>\n",
       "      <td>3.20 ± 0.61</td>\n",
       "      <td>0.24 ± 0.04</td>\n",
       "      <td>5.70 ± 0.89</td>\n",
       "      <td>0.17 ± 0.01</td>\n",
       "      <td>10910.28 ± 20.10</td>\n",
       "      <td>0.10 ± 0.01</td>\n",
       "      <td>18552.22 ± 153.56</td>\n",
       "      <td>13.85 ± 0.40</td>\n",
       "      <td>8.52 ± 0.45</td>\n",
       "      <td>11.185484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPW (no HPO)</th>\n",
       "      <td>0.24 ± 0.05</td>\n",
       "      <td>3.22 ± 0.62</td>\n",
       "      <td>0.23 ± 0.04</td>\n",
       "      <td>5.70 ± 0.89</td>\n",
       "      <td>0.22 ± 0.01</td>\n",
       "      <td>10956.57 ± 22.46</td>\n",
       "      <td>0.07 ± 0.01</td>\n",
       "      <td>18514.52 ± 149.16</td>\n",
       "      <td>14.05 ± 0.39</td>\n",
       "      <td>8.41 ± 0.43</td>\n",
       "      <td>11.229032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlexTENet (HPO)</th>\n",
       "      <td>0.04 ± 0.01</td>\n",
       "      <td>2.30 ± 0.26</td>\n",
       "      <td>0.21 ± 0.04</td>\n",
       "      <td>2.08 ± 0.11</td>\n",
       "      <td>0.53 ± 0.02</td>\n",
       "      <td>10329.63 ± 99.20</td>\n",
       "      <td>0.40 ± 0.06</td>\n",
       "      <td>17178.95 ± 729.35</td>\n",
       "      <td>11.55 ± 0.38</td>\n",
       "      <td>11.45 ± 0.39</td>\n",
       "      <td>11.496774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DragonNet (no HPO)</th>\n",
       "      <td>0.09 ± 0.02</td>\n",
       "      <td>2.23 ± 0.20</td>\n",
       "      <td>0.21 ± 0.04</td>\n",
       "      <td>2.13 ± 0.24</td>\n",
       "      <td>0.56 ± 0.03</td>\n",
       "      <td>10826.70 ± 148.05</td>\n",
       "      <td>0.44 ± 0.02</td>\n",
       "      <td>16396.12 ± 268.15</td>\n",
       "      <td>11.32 ± 0.38</td>\n",
       "      <td>12.75 ± 0.39</td>\n",
       "      <td>12.038710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DragonNet (HPO)</th>\n",
       "      <td>0.06 ± 0.02</td>\n",
       "      <td>2.11 ± 0.19</td>\n",
       "      <td>0.20 ± 0.04</td>\n",
       "      <td>2.16 ± 0.25</td>\n",
       "      <td>0.55 ± 0.03</td>\n",
       "      <td>10931.61 ± 154.75</td>\n",
       "      <td>0.47 ± 0.03</td>\n",
       "      <td>16446.54 ± 285.99</td>\n",
       "      <td>11.70 ± 0.39</td>\n",
       "      <td>12.97 ± 0.39</td>\n",
       "      <td>12.332258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RA Net (HPO)</th>\n",
       "      <td>0.07 ± 0.03</td>\n",
       "      <td>2.35 ± 0.25</td>\n",
       "      <td>0.20 ± 0.04</td>\n",
       "      <td>2.35 ± 0.19</td>\n",
       "      <td>0.74 ± 0.02</td>\n",
       "      <td>11737.30 ± 86.06</td>\n",
       "      <td>0.50 ± 0.04</td>\n",
       "      <td>18325.29 ± 432.77</td>\n",
       "      <td>14.39 ± 0.35</td>\n",
       "      <td>14.01 ± 0.39</td>\n",
       "      <td>14.201613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XNET (HPO)</th>\n",
       "      <td>0.06 ± 0.02</td>\n",
       "      <td>2.34 ± 0.26</td>\n",
       "      <td>0.20 ± 0.03</td>\n",
       "      <td>3.02 ± 0.34</td>\n",
       "      <td>0.63 ± 0.03</td>\n",
       "      <td>12106.41 ± 178.13</td>\n",
       "      <td>0.49 ± 0.08</td>\n",
       "      <td>20222.50 ± 999.97</td>\n",
       "      <td>15.55 ± 0.43</td>\n",
       "      <td>13.48 ± 0.43</td>\n",
       "      <td>14.514516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNet (no HPO)</th>\n",
       "      <td>0.04 ± 0.01</td>\n",
       "      <td>1.46 ± 0.17</td>\n",
       "      <td>0.21 ± 0.04</td>\n",
       "      <td>2.12 ± 0.13</td>\n",
       "      <td>0.79 ± 0.02</td>\n",
       "      <td>12265.98 ± 85.42</td>\n",
       "      <td>0.48 ± 0.08</td>\n",
       "      <td>19465.25 ± 1357.93</td>\n",
       "      <td>15.18 ± 0.40</td>\n",
       "      <td>13.92 ± 0.43</td>\n",
       "      <td>14.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X Learner (HPO)</th>\n",
       "      <td>0.03 ± 0.01</td>\n",
       "      <td>0.60 ± 0.08</td>\n",
       "      <td>0.16 ± 0.04</td>\n",
       "      <td>3.31 ± 0.51</td>\n",
       "      <td>0.84 ± 0.03</td>\n",
       "      <td>12146.30 ± 150.19</td>\n",
       "      <td>0.72 ± 0.03</td>\n",
       "      <td>20279.25 ± 490.56</td>\n",
       "      <td>15.35 ± 0.45</td>\n",
       "      <td>15.34 ± 0.45</td>\n",
       "      <td>15.343548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TarNet (no HPO)</th>\n",
       "      <td>0.06 ± 0.02</td>\n",
       "      <td>2.26 ± 0.20</td>\n",
       "      <td>0.21 ± 0.04</td>\n",
       "      <td>1.89 ± 0.15</td>\n",
       "      <td>0.90 ± 0.01</td>\n",
       "      <td>12001.89 ± 43.29</td>\n",
       "      <td>0.72 ± 0.01</td>\n",
       "      <td>18711.70 ± 163.65</td>\n",
       "      <td>14.39 ± 0.25</td>\n",
       "      <td>16.44 ± 0.31</td>\n",
       "      <td>15.414516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X Learner (no HPO)</th>\n",
       "      <td>0.07 ± 0.02</td>\n",
       "      <td>1.71 ± 0.31</td>\n",
       "      <td>0.19 ± 0.03</td>\n",
       "      <td>3.70 ± 0.62</td>\n",
       "      <td>0.83 ± 0.01</td>\n",
       "      <td>12280.55 ± 30.01</td>\n",
       "      <td>0.92 ± 0.01</td>\n",
       "      <td>21721.17 ± 161.43</td>\n",
       "      <td>17.19 ± 0.39</td>\n",
       "      <td>16.91 ± 0.39</td>\n",
       "      <td>17.048387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRF (no HPO)</th>\n",
       "      <td>0.07 ± 0.02</td>\n",
       "      <td>1.36 ± 0.30</td>\n",
       "      <td>0.18 ± 0.03</td>\n",
       "      <td>4.26 ± 0.69</td>\n",
       "      <td>0.81 ± 0.02</td>\n",
       "      <td>12175.72 ± 63.29</td>\n",
       "      <td>0.85 ± 0.02</td>\n",
       "      <td>21840.79 ± 159.43</td>\n",
       "      <td>17.66 ± 0.40</td>\n",
       "      <td>16.77 ± 0.40</td>\n",
       "      <td>17.217742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RA Net (no HPO)</th>\n",
       "      <td>0.07 ± 0.03</td>\n",
       "      <td>2.42 ± 0.22</td>\n",
       "      <td>0.20 ± 0.04</td>\n",
       "      <td>2.08 ± 0.19</td>\n",
       "      <td>0.96 ± 0.02</td>\n",
       "      <td>12861.38 ± 115.18</td>\n",
       "      <td>0.71 ± 0.04</td>\n",
       "      <td>20126.97 ± 407.46</td>\n",
       "      <td>17.45 ± 0.37</td>\n",
       "      <td>17.24 ± 0.40</td>\n",
       "      <td>17.340323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRF (HPO)</th>\n",
       "      <td>0.07 ± 0.02</td>\n",
       "      <td>1.32 ± 0.30</td>\n",
       "      <td>0.18 ± 0.03</td>\n",
       "      <td>3.67 ± 0.61</td>\n",
       "      <td>0.82 ± 0.02</td>\n",
       "      <td>12327.13 ± 64.40</td>\n",
       "      <td>0.85 ± 0.02</td>\n",
       "      <td>22913.91 ± 170.35</td>\n",
       "      <td>18.56 ± 0.45</td>\n",
       "      <td>16.74 ± 0.41</td>\n",
       "      <td>17.648387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S Learner (HPO)</th>\n",
       "      <td>0.03 ± 0.01</td>\n",
       "      <td>0.85 ± 0.13</td>\n",
       "      <td>0.20 ± 0.04</td>\n",
       "      <td>2.57 ± 0.41</td>\n",
       "      <td>0.97 ± 0.01</td>\n",
       "      <td>12658.65 ± 45.25</td>\n",
       "      <td>0.90 ± 0.02</td>\n",
       "      <td>21795.43 ± 177.17</td>\n",
       "      <td>17.57 ± 0.45</td>\n",
       "      <td>18.87 ± 0.42</td>\n",
       "      <td>18.219355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>0.04 ± 0.01</td>\n",
       "      <td>0.68 ± 0.11</td>\n",
       "      <td>0.44 ± 0.09</td>\n",
       "      <td>2.50 ± 0.39</td>\n",
       "      <td>0.99 ± 0.01</td>\n",
       "      <td>12806.75 ± 46.68</td>\n",
       "      <td>0.86 ± 0.01</td>\n",
       "      <td>21355.83 ± 161.05</td>\n",
       "      <td>17.61 ± 0.40</td>\n",
       "      <td>19.05 ± 0.42</td>\n",
       "      <td>18.332258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TarNet (HPO)</th>\n",
       "      <td>0.05 ± 0.02</td>\n",
       "      <td>2.20 ± 0.21</td>\n",
       "      <td>0.20 ± 0.04</td>\n",
       "      <td>1.82 ± 0.14</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>12877.87 ± 16.47</td>\n",
       "      <td>0.78 ± 0.01</td>\n",
       "      <td>19194.38 ± 183.28</td>\n",
       "      <td>17.19 ± 0.32</td>\n",
       "      <td>19.69 ± 0.37</td>\n",
       "      <td>18.440323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNet (HPO)</th>\n",
       "      <td>0.06 ± 0.01</td>\n",
       "      <td>1.22 ± 0.15</td>\n",
       "      <td>0.93 ± 0.02</td>\n",
       "      <td>7.50 ± 0.81</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>12877.87 ± 16.47</td>\n",
       "      <td>0.25 ± 0.03</td>\n",
       "      <td>15416.55 ± 453.59</td>\n",
       "      <td>17.92 ± 0.51</td>\n",
       "      <td>20.43 ± 0.52</td>\n",
       "      <td>19.177419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DR Learner (no HPO)</th>\n",
       "      <td>0.08 ± 0.04</td>\n",
       "      <td>1.68 ± 0.35</td>\n",
       "      <td>0.19 ± 0.04</td>\n",
       "      <td>3.90 ± 0.66</td>\n",
       "      <td>1.39 ± 0.28</td>\n",
       "      <td>26079.31 ± 4959.36</td>\n",
       "      <td>0.87 ± 0.03</td>\n",
       "      <td>22553.52 ± 245.21</td>\n",
       "      <td>20.69 ± 0.44</td>\n",
       "      <td>17.86 ± 0.38</td>\n",
       "      <td>19.275806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DR Learner (HPO)</th>\n",
       "      <td>0.04 ± 0.02</td>\n",
       "      <td>1.34 ± 0.29</td>\n",
       "      <td>0.17 ± 0.03</td>\n",
       "      <td>4.02 ± 0.67</td>\n",
       "      <td>1.20 ± 0.23</td>\n",
       "      <td>15980.05 ± 682.24</td>\n",
       "      <td>3.64 ± 2.78</td>\n",
       "      <td>22784.40 ± 538.61</td>\n",
       "      <td>21.39 ± 0.44</td>\n",
       "      <td>17.62 ± 0.41</td>\n",
       "      <td>19.506452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DML (HPO)</th>\n",
       "      <td>0.05 ± 0.01</td>\n",
       "      <td>1.48 ± 0.31</td>\n",
       "      <td>0.08 ± 0.01</td>\n",
       "      <td>4.53 ± 0.73</td>\n",
       "      <td>1.03 ± 0.01</td>\n",
       "      <td>12952.41 ± 43.82</td>\n",
       "      <td>1.05 ± 0.01</td>\n",
       "      <td>22991.83 ± 145.32</td>\n",
       "      <td>21.10 ± 0.41</td>\n",
       "      <td>20.87 ± 0.50</td>\n",
       "      <td>20.985484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S Learner (no HPO)</th>\n",
       "      <td>0.12 ± 0.05</td>\n",
       "      <td>2.23 ± 0.28</td>\n",
       "      <td>0.28 ± 0.05</td>\n",
       "      <td>3.91 ± 0.68</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>12877.87 ± 16.47</td>\n",
       "      <td>1.03 ± 0.00</td>\n",
       "      <td>22677.62 ± 129.20</td>\n",
       "      <td>20.00 ± 0.43</td>\n",
       "      <td>22.54 ± 0.44</td>\n",
       "      <td>21.270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DML (no HPO)</th>\n",
       "      <td>0.05 ± 0.02</td>\n",
       "      <td>1.47 ± 0.32</td>\n",
       "      <td>0.09 ± 0.02</td>\n",
       "      <td>4.40 ± 0.72</td>\n",
       "      <td>1.12 ± 0.02</td>\n",
       "      <td>15119.73 ± 150.85</td>\n",
       "      <td>1.02 ± 0.01</td>\n",
       "      <td>23115.62 ± 149.99</td>\n",
       "      <td>23.14 ± 0.45</td>\n",
       "      <td>20.55 ± 0.54</td>\n",
       "      <td>21.846774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XNET (no HPO)</th>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>4.95 ± 0.51</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>7.96 ± 0.82</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>12877.87 ± 16.47</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>22700.94 ± 129.66</td>\n",
       "      <td>24.30 ± 0.20</td>\n",
       "      <td>26.78 ± 0.11</td>\n",
       "      <td>25.543548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlexTENet (no HPO)</th>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>4.95 ± 0.51</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>7.96 ± 0.82</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>12877.87 ± 16.47</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>22700.94 ± 129.66</td>\n",
       "      <td>24.32 ± 0.19</td>\n",
       "      <td>26.78 ± 0.11</td>\n",
       "      <td>25.550000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                       ACIC 2016                      IHDP  \\\n",
       "metric                      ate_rel_err    cate_pehe  ate_rel_err   \n",
       "method                                                              \n",
       "CausalPFN                   0.05 ± 0.01  0.92 ± 0.11  0.20 ± 0.04   \n",
       "T Learner (HPO)             0.03 ± 0.01  0.76 ± 0.07  0.21 ± 0.04   \n",
       "DA Learner (no HPO)         0.09 ± 0.03  1.88 ± 0.24  0.22 ± 0.04   \n",
       "DA Learner (HPO)            0.03 ± 0.01  0.72 ± 0.08  0.23 ± 0.04   \n",
       "T Learner (no HPO)          0.11 ± 0.03  2.06 ± 0.20  0.22 ± 0.04   \n",
       "IPW (HPO)                   0.21 ± 0.05  3.20 ± 0.61  0.24 ± 0.04   \n",
       "IPW (no HPO)                0.24 ± 0.05  3.22 ± 0.62  0.23 ± 0.04   \n",
       "FlexTENet (HPO)             0.04 ± 0.01  2.30 ± 0.26  0.21 ± 0.04   \n",
       "DragonNet (no HPO)          0.09 ± 0.02  2.23 ± 0.20  0.21 ± 0.04   \n",
       "DragonNet (HPO)             0.06 ± 0.02  2.11 ± 0.19  0.20 ± 0.04   \n",
       "RA Net (HPO)                0.07 ± 0.03  2.35 ± 0.25  0.20 ± 0.04   \n",
       "XNET (HPO)                  0.06 ± 0.02  2.34 ± 0.26  0.20 ± 0.03   \n",
       "SNet (no HPO)               0.04 ± 0.01  1.46 ± 0.17  0.21 ± 0.04   \n",
       "X Learner (HPO)             0.03 ± 0.01  0.60 ± 0.08  0.16 ± 0.04   \n",
       "TarNet (no HPO)             0.06 ± 0.02  2.26 ± 0.20  0.21 ± 0.04   \n",
       "X Learner (no HPO)          0.07 ± 0.02  1.71 ± 0.31  0.19 ± 0.03   \n",
       "GRF (no HPO)                0.07 ± 0.02  1.36 ± 0.30  0.18 ± 0.03   \n",
       "RA Net (no HPO)             0.07 ± 0.03  2.42 ± 0.22  0.20 ± 0.04   \n",
       "GRF (HPO)                   0.07 ± 0.02  1.32 ± 0.30  0.18 ± 0.03   \n",
       "S Learner (HPO)             0.03 ± 0.01  0.85 ± 0.13  0.20 ± 0.04   \n",
       "BART                        0.04 ± 0.01  0.68 ± 0.11  0.44 ± 0.09   \n",
       "TarNet (HPO)                0.05 ± 0.02  2.20 ± 0.21  0.20 ± 0.04   \n",
       "SNet (HPO)                  0.06 ± 0.01  1.22 ± 0.15  0.93 ± 0.02   \n",
       "Forest DR Learner (no HPO)  0.08 ± 0.04  1.68 ± 0.35  0.19 ± 0.04   \n",
       "Forest DR Learner (HPO)     0.04 ± 0.02  1.34 ± 0.29  0.17 ± 0.03   \n",
       "Forest DML (HPO)            0.05 ± 0.01  1.48 ± 0.31  0.08 ± 0.01   \n",
       "S Learner (no HPO)          0.12 ± 0.05  2.23 ± 0.28  0.28 ± 0.05   \n",
       "Forest DML (no HPO)         0.05 ± 0.02  1.47 ± 0.32  0.09 ± 0.02   \n",
       "XNET (no HPO)               1.00 ± 0.00  4.95 ± 0.51  1.00 ± 0.00   \n",
       "FlexTENet (no HPO)          1.00 ± 0.00  4.95 ± 0.51  1.00 ± 0.00   \n",
       "\n",
       "dataset                                 RealCause Lalonde CPS  \\\n",
       "metric                        cate_pehe           ate_rel_err   \n",
       "method                                                          \n",
       "CausalPFN                   0.58 ± 0.07           0.13 ± 0.01   \n",
       "T Learner (HPO)             1.73 ± 0.30           0.24 ± 0.02   \n",
       "DA Learner (no HPO)         2.98 ± 0.51           0.22 ± 0.01   \n",
       "DA Learner (HPO)            2.07 ± 0.36           0.27 ± 0.02   \n",
       "T Learner (no HPO)          2.94 ± 0.49           0.40 ± 0.01   \n",
       "IPW (HPO)                   5.70 ± 0.89           0.17 ± 0.01   \n",
       "IPW (no HPO)                5.70 ± 0.89           0.22 ± 0.01   \n",
       "FlexTENet (HPO)             2.08 ± 0.11           0.53 ± 0.02   \n",
       "DragonNet (no HPO)          2.13 ± 0.24           0.56 ± 0.03   \n",
       "DragonNet (HPO)             2.16 ± 0.25           0.55 ± 0.03   \n",
       "RA Net (HPO)                2.35 ± 0.19           0.74 ± 0.02   \n",
       "XNET (HPO)                  3.02 ± 0.34           0.63 ± 0.03   \n",
       "SNet (no HPO)               2.12 ± 0.13           0.79 ± 0.02   \n",
       "X Learner (HPO)             3.31 ± 0.51           0.84 ± 0.03   \n",
       "TarNet (no HPO)             1.89 ± 0.15           0.90 ± 0.01   \n",
       "X Learner (no HPO)          3.70 ± 0.62           0.83 ± 0.01   \n",
       "GRF (no HPO)                4.26 ± 0.69           0.81 ± 0.02   \n",
       "RA Net (no HPO)             2.08 ± 0.19           0.96 ± 0.02   \n",
       "GRF (HPO)                   3.67 ± 0.61           0.82 ± 0.02   \n",
       "S Learner (HPO)             2.57 ± 0.41           0.97 ± 0.01   \n",
       "BART                        2.50 ± 0.39           0.99 ± 0.01   \n",
       "TarNet (HPO)                1.82 ± 0.14           1.00 ± 0.00   \n",
       "SNet (HPO)                  7.50 ± 0.81           1.00 ± 0.00   \n",
       "Forest DR Learner (no HPO)  3.90 ± 0.66           1.39 ± 0.28   \n",
       "Forest DR Learner (HPO)     4.02 ± 0.67           1.20 ± 0.23   \n",
       "Forest DML (HPO)            4.53 ± 0.73           1.03 ± 0.01   \n",
       "S Learner (no HPO)          3.91 ± 0.68           1.00 ± 0.00   \n",
       "Forest DML (no HPO)         4.40 ± 0.72           1.12 ± 0.02   \n",
       "XNET (no HPO)               7.96 ± 0.82           1.00 ± 0.00   \n",
       "FlexTENet (no HPO)          7.96 ± 0.82           1.00 ± 0.00   \n",
       "\n",
       "dataset                                        RealCause Lalonde PSID  \\\n",
       "metric                               cate_pehe            ate_rel_err   \n",
       "method                                                                  \n",
       "CausalPFN                      8955.82 ± 20.92            0.22 ± 0.02   \n",
       "T Learner (HPO)                9223.92 ± 36.35            0.16 ± 0.03   \n",
       "DA Learner (no HPO)            9011.68 ± 23.47            0.08 ± 0.01   \n",
       "DA Learner (HPO)               9387.08 ± 56.81            0.20 ± 0.03   \n",
       "T Learner (no HPO)             9291.93 ± 23.20            0.07 ± 0.01   \n",
       "IPW (HPO)                     10910.28 ± 20.10            0.10 ± 0.01   \n",
       "IPW (no HPO)                  10956.57 ± 22.46            0.07 ± 0.01   \n",
       "FlexTENet (HPO)               10329.63 ± 99.20            0.40 ± 0.06   \n",
       "DragonNet (no HPO)           10826.70 ± 148.05            0.44 ± 0.02   \n",
       "DragonNet (HPO)              10931.61 ± 154.75            0.47 ± 0.03   \n",
       "RA Net (HPO)                  11737.30 ± 86.06            0.50 ± 0.04   \n",
       "XNET (HPO)                   12106.41 ± 178.13            0.49 ± 0.08   \n",
       "SNet (no HPO)                 12265.98 ± 85.42            0.48 ± 0.08   \n",
       "X Learner (HPO)              12146.30 ± 150.19            0.72 ± 0.03   \n",
       "TarNet (no HPO)               12001.89 ± 43.29            0.72 ± 0.01   \n",
       "X Learner (no HPO)            12280.55 ± 30.01            0.92 ± 0.01   \n",
       "GRF (no HPO)                  12175.72 ± 63.29            0.85 ± 0.02   \n",
       "RA Net (no HPO)              12861.38 ± 115.18            0.71 ± 0.04   \n",
       "GRF (HPO)                     12327.13 ± 64.40            0.85 ± 0.02   \n",
       "S Learner (HPO)               12658.65 ± 45.25            0.90 ± 0.02   \n",
       "BART                          12806.75 ± 46.68            0.86 ± 0.01   \n",
       "TarNet (HPO)                  12877.87 ± 16.47            0.78 ± 0.01   \n",
       "SNet (HPO)                    12877.87 ± 16.47            0.25 ± 0.03   \n",
       "Forest DR Learner (no HPO)  26079.31 ± 4959.36            0.87 ± 0.03   \n",
       "Forest DR Learner (HPO)      15980.05 ± 682.24            3.64 ± 2.78   \n",
       "Forest DML (HPO)              12952.41 ± 43.82            1.05 ± 0.01   \n",
       "S Learner (no HPO)            12877.87 ± 16.47            1.03 ± 0.00   \n",
       "Forest DML (no HPO)          15119.73 ± 150.85            1.02 ± 0.01   \n",
       "XNET (no HPO)                 12877.87 ± 16.47            1.00 ± 0.00   \n",
       "FlexTENet (no HPO)            12877.87 ± 16.47            1.00 ± 0.00   \n",
       "\n",
       "dataset                                                overall                 \\\n",
       "metric                               cate_pehe cate_rank ± ste ate_rank ± ste   \n",
       "method                                                                          \n",
       "CausalPFN                    14402.01 ± 198.32     3.85 ± 0.20    8.69 ± 0.41   \n",
       "T Learner (HPO)              15162.79 ± 462.18     6.46 ± 0.34    8.45 ± 0.40   \n",
       "DA Learner (no HPO)          13959.67 ± 192.75     6.87 ± 0.42    8.11 ± 0.41   \n",
       "DA Learner (HPO)             14548.75 ± 235.63     6.60 ± 0.35    9.48 ± 0.41   \n",
       "T Learner (no HPO)           13910.37 ± 176.85     7.52 ± 0.39    9.12 ± 0.39   \n",
       "IPW (HPO)                    18552.22 ± 153.56    13.85 ± 0.40    8.52 ± 0.45   \n",
       "IPW (no HPO)                 18514.52 ± 149.16    14.05 ± 0.39    8.41 ± 0.43   \n",
       "FlexTENet (HPO)              17178.95 ± 729.35    11.55 ± 0.38   11.45 ± 0.39   \n",
       "DragonNet (no HPO)           16396.12 ± 268.15    11.32 ± 0.38   12.75 ± 0.39   \n",
       "DragonNet (HPO)              16446.54 ± 285.99    11.70 ± 0.39   12.97 ± 0.39   \n",
       "RA Net (HPO)                 18325.29 ± 432.77    14.39 ± 0.35   14.01 ± 0.39   \n",
       "XNET (HPO)                   20222.50 ± 999.97    15.55 ± 0.43   13.48 ± 0.43   \n",
       "SNet (no HPO)               19465.25 ± 1357.93    15.18 ± 0.40   13.92 ± 0.43   \n",
       "X Learner (HPO)              20279.25 ± 490.56    15.35 ± 0.45   15.34 ± 0.45   \n",
       "TarNet (no HPO)              18711.70 ± 163.65    14.39 ± 0.25   16.44 ± 0.31   \n",
       "X Learner (no HPO)           21721.17 ± 161.43    17.19 ± 0.39   16.91 ± 0.39   \n",
       "GRF (no HPO)                 21840.79 ± 159.43    17.66 ± 0.40   16.77 ± 0.40   \n",
       "RA Net (no HPO)              20126.97 ± 407.46    17.45 ± 0.37   17.24 ± 0.40   \n",
       "GRF (HPO)                    22913.91 ± 170.35    18.56 ± 0.45   16.74 ± 0.41   \n",
       "S Learner (HPO)              21795.43 ± 177.17    17.57 ± 0.45   18.87 ± 0.42   \n",
       "BART                         21355.83 ± 161.05    17.61 ± 0.40   19.05 ± 0.42   \n",
       "TarNet (HPO)                 19194.38 ± 183.28    17.19 ± 0.32   19.69 ± 0.37   \n",
       "SNet (HPO)                   15416.55 ± 453.59    17.92 ± 0.51   20.43 ± 0.52   \n",
       "Forest DR Learner (no HPO)   22553.52 ± 245.21    20.69 ± 0.44   17.86 ± 0.38   \n",
       "Forest DR Learner (HPO)      22784.40 ± 538.61    21.39 ± 0.44   17.62 ± 0.41   \n",
       "Forest DML (HPO)             22991.83 ± 145.32    21.10 ± 0.41   20.87 ± 0.50   \n",
       "S Learner (no HPO)           22677.62 ± 129.20    20.00 ± 0.43   22.54 ± 0.44   \n",
       "Forest DML (no HPO)          23115.62 ± 149.99    23.14 ± 0.45   20.55 ± 0.54   \n",
       "XNET (no HPO)                22700.94 ± 129.66    24.30 ± 0.20   26.78 ± 0.11   \n",
       "FlexTENet (no HPO)           22700.94 ± 129.66    24.32 ± 0.19   26.78 ± 0.11   \n",
       "\n",
       "dataset                                \n",
       "metric                           rank  \n",
       "method                                 \n",
       "CausalPFN                    6.272581  \n",
       "T Learner (HPO)              7.456452  \n",
       "DA Learner (no HPO)          7.490323  \n",
       "DA Learner (HPO)             8.040323  \n",
       "T Learner (no HPO)           8.319355  \n",
       "IPW (HPO)                   11.185484  \n",
       "IPW (no HPO)                11.229032  \n",
       "FlexTENet (HPO)             11.496774  \n",
       "DragonNet (no HPO)          12.038710  \n",
       "DragonNet (HPO)             12.332258  \n",
       "RA Net (HPO)                14.201613  \n",
       "XNET (HPO)                  14.514516  \n",
       "SNet (no HPO)               14.550000  \n",
       "X Learner (HPO)             15.343548  \n",
       "TarNet (no HPO)             15.414516  \n",
       "X Learner (no HPO)          17.048387  \n",
       "GRF (no HPO)                17.217742  \n",
       "RA Net (no HPO)             17.340323  \n",
       "GRF (HPO)                   17.648387  \n",
       "S Learner (HPO)             18.219355  \n",
       "BART                        18.332258  \n",
       "TarNet (HPO)                18.440323  \n",
       "SNet (HPO)                  19.177419  \n",
       "Forest DR Learner (no HPO)  19.275806  \n",
       "Forest DR Learner (HPO)     19.506452  \n",
       "Forest DML (HPO)            20.985484  \n",
       "S Learner (no HPO)          21.270968  \n",
       "Forest DML (no HPO)         21.846774  \n",
       "XNET (no HPO)               25.543548  \n",
       "FlexTENet (no HPO)          25.550000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ate_errors = defaultdict(list)\n",
    "cate_pehes = defaultdict(list)\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dset_result = load_all_results(dataset_name, causal_effect_path)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"pehe\"])\n",
    "        for fold_idx in range(num_realizations):\n",
    "            pehe = float(all_rows[\"pehe\"][fold_idx])\n",
    "            ate_rel_err = float(all_rows[\"ate_rel_err\"][fold_idx])\n",
    "            ate_errors[method].append(ate_rel_err)\n",
    "            cate_pehes[method].append(pehe)\n",
    "\n",
    "\n",
    "def get_ranks(res: dict):\n",
    "    ranks = {}\n",
    "    ranks_ste = {}\n",
    "    for method in methods_to_show:\n",
    "        all_len = len(res[method])\n",
    "        all_ranks = []\n",
    "        for idx in range(all_len):\n",
    "            rank = 0\n",
    "            for other_methods in methods_to_show:\n",
    "                our_res = res[method][idx]\n",
    "                other_res = res[other_methods][idx]\n",
    "                rank += our_res >= other_res\n",
    "            all_ranks.append(rank)\n",
    "        ranks[method] = sum(all_ranks) / all_len\n",
    "        ranks_ste[method] = np.std(all_ranks) / np.sqrt(all_len)\n",
    "    return ranks, ranks_ste\n",
    "\n",
    "\n",
    "cate_ranks, cate_ranks_ste = get_ranks(cate_pehes)\n",
    "ate_ranks, ate_ranks_ste = get_ranks(ate_errors)\n",
    "\n",
    "# add a multicolumn to causal_effect_errors called \"overall\"\n",
    "causal_effect_errors[(\"overall\", \"cate_rank ± ste\")] = pd.Series(\n",
    "    {method: f\"{cate_ranks[method]:.2f} ± {cate_ranks_ste[method]:.2f}\" for method in methods_to_show}\n",
    ")\n",
    "causal_effect_errors[(\"overall\", \"ate_rank ± ste\")] = pd.Series(\n",
    "    {method: f\"{ate_ranks[method]:.2f} ± {ate_ranks_ste[method]:.2f}\" for method in methods_to_show}\n",
    ")\n",
    "\n",
    "causal_effect_errors[(\"overall\", \"rank\")] = (pd.Series(cate_ranks) + pd.Series(ate_ranks)) / 2\n",
    "# sort rows by rank\n",
    "causal_effect_errors = causal_effect_errors.sort_values(by=(\"overall\", \"rank\"))\n",
    "causal_effect_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c39b26",
   "metadata": {},
   "source": [
    "Compute average time for each method and add that to the dataframe of times and sort according to that average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7318cc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ACIC 2016</th>\n",
       "      <th colspan=\"2\" halign=\"left\">IHDP</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RealCause Lalonde CPS</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RealCause Lalonde PSID</th>\n",
       "      <th colspan=\"2\" halign=\"left\">overall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ate_time</th>\n",
       "      <th>cate_time</th>\n",
       "      <th>ate_time</th>\n",
       "      <th>cate_time</th>\n",
       "      <th>ate_time</th>\n",
       "      <th>cate_time</th>\n",
       "      <th>ate_time</th>\n",
       "      <th>cate_time</th>\n",
       "      <th>ate_time</th>\n",
       "      <th>cate_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FlexTENet (no HPO)</th>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPW (no HPO)</th>\n",
       "      <td>0.064492</td>\n",
       "      <td>0.059753</td>\n",
       "      <td>0.127351</td>\n",
       "      <td>0.125096</td>\n",
       "      <td>0.042458</td>\n",
       "      <td>0.040139</td>\n",
       "      <td>0.081067</td>\n",
       "      <td>0.081708</td>\n",
       "      <td>0.077551</td>\n",
       "      <td>0.078507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S Learner (no HPO)</th>\n",
       "      <td>0.075717</td>\n",
       "      <td>0.075550</td>\n",
       "      <td>0.466148</td>\n",
       "      <td>0.454180</td>\n",
       "      <td>0.021364</td>\n",
       "      <td>0.021157</td>\n",
       "      <td>0.111352</td>\n",
       "      <td>0.115120</td>\n",
       "      <td>0.115654</td>\n",
       "      <td>0.118159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T Learner (no HPO)</th>\n",
       "      <td>0.120445</td>\n",
       "      <td>0.116674</td>\n",
       "      <td>0.916476</td>\n",
       "      <td>0.883767</td>\n",
       "      <td>0.044261</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.248036</td>\n",
       "      <td>0.245745</td>\n",
       "      <td>0.252443</td>\n",
       "      <td>0.247548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA Learner (no HPO)</th>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.283248</td>\n",
       "      <td>2.078678</td>\n",
       "      <td>2.076818</td>\n",
       "      <td>0.088429</td>\n",
       "      <td>0.089609</td>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.535804</td>\n",
       "      <td>0.535191</td>\n",
       "      <td>0.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CausalPFN</th>\n",
       "      <td>1.778429</td>\n",
       "      <td>0.630637</td>\n",
       "      <td>1.876200</td>\n",
       "      <td>0.673687</td>\n",
       "      <td>3.061373</td>\n",
       "      <td>0.818941</td>\n",
       "      <td>2.966882</td>\n",
       "      <td>0.485863</td>\n",
       "      <td>2.534572</td>\n",
       "      <td>0.555087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRF (no HPO)</th>\n",
       "      <td>3.273261</td>\n",
       "      <td>1.051030</td>\n",
       "      <td>4.624504</td>\n",
       "      <td>0.596345</td>\n",
       "      <td>2.189785</td>\n",
       "      <td>0.708011</td>\n",
       "      <td>3.268610</td>\n",
       "      <td>0.510705</td>\n",
       "      <td>3.219808</td>\n",
       "      <td>0.579403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X Learner (no HPO)</th>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.328857</td>\n",
       "      <td>2.628365</td>\n",
       "      <td>2.585276</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.105298</td>\n",
       "      <td>0.684558</td>\n",
       "      <td>0.691134</td>\n",
       "      <td>0.676213</td>\n",
       "      <td>0.679522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TarNet (no HPO)</th>\n",
       "      <td>1.276442</td>\n",
       "      <td>1.293894</td>\n",
       "      <td>5.316109</td>\n",
       "      <td>5.866852</td>\n",
       "      <td>0.231617</td>\n",
       "      <td>0.314323</td>\n",
       "      <td>1.713454</td>\n",
       "      <td>1.102849</td>\n",
       "      <td>1.568243</td>\n",
       "      <td>1.053019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DragonNet (no HPO)</th>\n",
       "      <td>1.487617</td>\n",
       "      <td>1.506751</td>\n",
       "      <td>4.642988</td>\n",
       "      <td>4.659946</td>\n",
       "      <td>0.599383</td>\n",
       "      <td>1.168565</td>\n",
       "      <td>3.027199</td>\n",
       "      <td>2.607162</td>\n",
       "      <td>2.587719</td>\n",
       "      <td>1.916093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRF (HPO)</th>\n",
       "      <td>3.351523</td>\n",
       "      <td>3.142087</td>\n",
       "      <td>4.533691</td>\n",
       "      <td>4.573715</td>\n",
       "      <td>2.226246</td>\n",
       "      <td>2.130062</td>\n",
       "      <td>3.168451</td>\n",
       "      <td>3.088848</td>\n",
       "      <td>3.140710</td>\n",
       "      <td>3.045631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XNET (no HPO)</th>\n",
       "      <td>4.372944</td>\n",
       "      <td>3.655362</td>\n",
       "      <td>12.249859</td>\n",
       "      <td>12.194790</td>\n",
       "      <td>1.573581</td>\n",
       "      <td>1.346342</td>\n",
       "      <td>4.423722</td>\n",
       "      <td>3.598758</td>\n",
       "      <td>4.341404</td>\n",
       "      <td>3.586190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RA Net (no HPO)</th>\n",
       "      <td>5.002618</td>\n",
       "      <td>4.501656</td>\n",
       "      <td>13.268286</td>\n",
       "      <td>14.892828</td>\n",
       "      <td>1.051872</td>\n",
       "      <td>1.129872</td>\n",
       "      <td>4.269286</td>\n",
       "      <td>4.681562</td>\n",
       "      <td>3.828014</td>\n",
       "      <td>3.883391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DR Learner (no HPO)</th>\n",
       "      <td>483.112359</td>\n",
       "      <td>424.962053</td>\n",
       "      <td>28.169275</td>\n",
       "      <td>25.474570</td>\n",
       "      <td>3.271908</td>\n",
       "      <td>2.920928</td>\n",
       "      <td>4.277007</td>\n",
       "      <td>3.989793</td>\n",
       "      <td>4.148781</td>\n",
       "      <td>3.892557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DML (no HPO)</th>\n",
       "      <td>630.151684</td>\n",
       "      <td>554.478033</td>\n",
       "      <td>36.546051</td>\n",
       "      <td>32.682874</td>\n",
       "      <td>4.319115</td>\n",
       "      <td>3.836490</td>\n",
       "      <td>5.377914</td>\n",
       "      <td>5.007255</td>\n",
       "      <td>5.634726</td>\n",
       "      <td>5.222570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNet (no HPO)</th>\n",
       "      <td>4.015937</td>\n",
       "      <td>3.883040</td>\n",
       "      <td>10.951375</td>\n",
       "      <td>11.515392</td>\n",
       "      <td>0.354591</td>\n",
       "      <td>0.578330</td>\n",
       "      <td>7.347283</td>\n",
       "      <td>5.761024</td>\n",
       "      <td>7.223473</td>\n",
       "      <td>5.604429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XNET (HPO)</th>\n",
       "      <td>6.903680</td>\n",
       "      <td>6.246199</td>\n",
       "      <td>19.711522</td>\n",
       "      <td>21.350684</td>\n",
       "      <td>2.749048</td>\n",
       "      <td>2.440719</td>\n",
       "      <td>7.225780</td>\n",
       "      <td>6.218904</td>\n",
       "      <td>6.706197</td>\n",
       "      <td>5.953733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlexTENet (HPO)</th>\n",
       "      <td>5.264130</td>\n",
       "      <td>5.236548</td>\n",
       "      <td>15.552939</td>\n",
       "      <td>16.792147</td>\n",
       "      <td>1.357712</td>\n",
       "      <td>1.657927</td>\n",
       "      <td>5.196586</td>\n",
       "      <td>7.300758</td>\n",
       "      <td>5.094625</td>\n",
       "      <td>7.168991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>5.016562</td>\n",
       "      <td>4.509301</td>\n",
       "      <td>6.438867</td>\n",
       "      <td>5.668311</td>\n",
       "      <td>11.458935</td>\n",
       "      <td>9.506232</td>\n",
       "      <td>10.516291</td>\n",
       "      <td>9.264590</td>\n",
       "      <td>10.007730</td>\n",
       "      <td>8.637354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TarNet (HPO)</th>\n",
       "      <td>23.137612</td>\n",
       "      <td>23.758678</td>\n",
       "      <td>92.663927</td>\n",
       "      <td>108.224995</td>\n",
       "      <td>0.210069</td>\n",
       "      <td>0.337117</td>\n",
       "      <td>28.186210</td>\n",
       "      <td>20.035166</td>\n",
       "      <td>25.688087</td>\n",
       "      <td>18.034589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DragonNet (HPO)</th>\n",
       "      <td>25.612350</td>\n",
       "      <td>26.379348</td>\n",
       "      <td>101.961289</td>\n",
       "      <td>104.886658</td>\n",
       "      <td>9.029205</td>\n",
       "      <td>10.432764</td>\n",
       "      <td>36.160391</td>\n",
       "      <td>27.710479</td>\n",
       "      <td>32.526312</td>\n",
       "      <td>26.099312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RA Net (HPO)</th>\n",
       "      <td>57.229469</td>\n",
       "      <td>55.611749</td>\n",
       "      <td>241.409582</td>\n",
       "      <td>248.408577</td>\n",
       "      <td>26.495504</td>\n",
       "      <td>28.642944</td>\n",
       "      <td>62.936859</td>\n",
       "      <td>51.355192</td>\n",
       "      <td>56.764950</td>\n",
       "      <td>47.459009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNet (HPO)</th>\n",
       "      <td>79.072716</td>\n",
       "      <td>79.283878</td>\n",
       "      <td>103.045946</td>\n",
       "      <td>106.030599</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.628990</td>\n",
       "      <td>151.726262</td>\n",
       "      <td>115.182332</td>\n",
       "      <td>81.960905</td>\n",
       "      <td>88.576926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPW (HPO)</th>\n",
       "      <td>187.445818</td>\n",
       "      <td>187.450784</td>\n",
       "      <td>1205.191209</td>\n",
       "      <td>1205.171748</td>\n",
       "      <td>55.113963</td>\n",
       "      <td>55.112547</td>\n",
       "      <td>335.441295</td>\n",
       "      <td>335.388256</td>\n",
       "      <td>336.480014</td>\n",
       "      <td>336.472630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S Learner (HPO)</th>\n",
       "      <td>187.785203</td>\n",
       "      <td>188.501705</td>\n",
       "      <td>1205.187853</td>\n",
       "      <td>1205.113190</td>\n",
       "      <td>55.193442</td>\n",
       "      <td>55.201033</td>\n",
       "      <td>335.498217</td>\n",
       "      <td>335.625539</td>\n",
       "      <td>336.523395</td>\n",
       "      <td>336.522268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T Learner (HPO)</th>\n",
       "      <td>375.362602</td>\n",
       "      <td>375.225016</td>\n",
       "      <td>1805.152096</td>\n",
       "      <td>1807.565992</td>\n",
       "      <td>110.639473</td>\n",
       "      <td>110.636495</td>\n",
       "      <td>358.069995</td>\n",
       "      <td>358.009164</td>\n",
       "      <td>336.598740</td>\n",
       "      <td>336.587934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X Learner (HPO)</th>\n",
       "      <td>562.901286</td>\n",
       "      <td>562.840737</td>\n",
       "      <td>3431.698731</td>\n",
       "      <td>3434.412390</td>\n",
       "      <td>166.020475</td>\n",
       "      <td>165.984879</td>\n",
       "      <td>801.324017</td>\n",
       "      <td>801.266232</td>\n",
       "      <td>673.518492</td>\n",
       "      <td>673.465243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DR Learner (HPO)</th>\n",
       "      <td>826.427797</td>\n",
       "      <td>774.391367</td>\n",
       "      <td>1711.189979</td>\n",
       "      <td>1708.428262</td>\n",
       "      <td>104.804439</td>\n",
       "      <td>104.389961</td>\n",
       "      <td>674.208057</td>\n",
       "      <td>673.787195</td>\n",
       "      <td>675.454211</td>\n",
       "      <td>675.156030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forest DML (HPO)</th>\n",
       "      <td>1230.262155</td>\n",
       "      <td>1117.191892</td>\n",
       "      <td>2463.427748</td>\n",
       "      <td>2457.302990</td>\n",
       "      <td>115.939603</td>\n",
       "      <td>115.355165</td>\n",
       "      <td>679.377478</td>\n",
       "      <td>678.771401</td>\n",
       "      <td>680.568412</td>\n",
       "      <td>679.731633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA Learner (HPO)</th>\n",
       "      <td>750.537069</td>\n",
       "      <td>750.231691</td>\n",
       "      <td>4819.948724</td>\n",
       "      <td>4820.032942</td>\n",
       "      <td>193.046603</td>\n",
       "      <td>192.999544</td>\n",
       "      <td>1341.972529</td>\n",
       "      <td>1341.984959</td>\n",
       "      <td>1346.093917</td>\n",
       "      <td>1346.098121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                       ACIC 2016                      IHDP  \\\n",
       "                               ate_time    cate_time     ate_time   \n",
       "method                                                              \n",
       "FlexTENet (no HPO)             0.000857     0.000758     0.001603   \n",
       "IPW (no HPO)                   0.064492     0.059753     0.127351   \n",
       "S Learner (no HPO)             0.075717     0.075550     0.466148   \n",
       "T Learner (no HPO)             0.120445     0.116674     0.916476   \n",
       "DA Learner (no HPO)            0.279503     0.283248     2.078678   \n",
       "CausalPFN                      1.778429     0.630637     1.876200   \n",
       "GRF (no HPO)                   3.273261     1.051030     4.624504   \n",
       "X Learner (no HPO)             0.325441     0.328857     2.628365   \n",
       "TarNet (no HPO)                1.276442     1.293894     5.316109   \n",
       "DragonNet (no HPO)             1.487617     1.506751     4.642988   \n",
       "GRF (HPO)                      3.351523     3.142087     4.533691   \n",
       "XNET (no HPO)                  4.372944     3.655362    12.249859   \n",
       "RA Net (no HPO)                5.002618     4.501656    13.268286   \n",
       "Forest DR Learner (no HPO)   483.112359   424.962053    28.169275   \n",
       "Forest DML (no HPO)          630.151684   554.478033    36.546051   \n",
       "SNet (no HPO)                  4.015937     3.883040    10.951375   \n",
       "XNET (HPO)                     6.903680     6.246199    19.711522   \n",
       "FlexTENet (HPO)                5.264130     5.236548    15.552939   \n",
       "BART                           5.016562     4.509301     6.438867   \n",
       "TarNet (HPO)                  23.137612    23.758678    92.663927   \n",
       "DragonNet (HPO)               25.612350    26.379348   101.961289   \n",
       "RA Net (HPO)                  57.229469    55.611749   241.409582   \n",
       "SNet (HPO)                    79.072716    79.283878   103.045946   \n",
       "IPW (HPO)                    187.445818   187.450784  1205.191209   \n",
       "S Learner (HPO)              187.785203   188.501705  1205.187853   \n",
       "T Learner (HPO)              375.362602   375.225016  1805.152096   \n",
       "X Learner (HPO)              562.901286   562.840737  3431.698731   \n",
       "Forest DR Learner (HPO)      826.427797   774.391367  1711.189979   \n",
       "Forest DML (HPO)            1230.262155  1117.191892  2463.427748   \n",
       "DA Learner (HPO)             750.537069   750.231691  4819.948724   \n",
       "\n",
       "dataset                                 RealCause Lalonde CPS              \\\n",
       "                              cate_time              ate_time   cate_time   \n",
       "method                                                                      \n",
       "FlexTENet (no HPO)             0.001670              0.000160    0.000218   \n",
       "IPW (no HPO)                   0.125096              0.042458    0.040139   \n",
       "S Learner (no HPO)             0.454180              0.021364    0.021157   \n",
       "T Learner (no HPO)             0.883767              0.044261    0.043567   \n",
       "DA Learner (no HPO)            2.076818              0.088429    0.089609   \n",
       "CausalPFN                      0.673687              3.061373    0.818941   \n",
       "GRF (no HPO)                   0.596345              2.189785    0.708011   \n",
       "X Learner (no HPO)             2.585276              0.105608    0.105298   \n",
       "TarNet (no HPO)                5.866852              0.231617    0.314323   \n",
       "DragonNet (no HPO)             4.659946              0.599383    1.168565   \n",
       "GRF (HPO)                      4.573715              2.226246    2.130062   \n",
       "XNET (no HPO)                 12.194790              1.573581    1.346342   \n",
       "RA Net (no HPO)               14.892828              1.051872    1.129872   \n",
       "Forest DR Learner (no HPO)    25.474570              3.271908    2.920928   \n",
       "Forest DML (no HPO)           32.682874              4.319115    3.836490   \n",
       "SNet (no HPO)                 11.515392              0.354591    0.578330   \n",
       "XNET (HPO)                    21.350684              2.749048    2.440719   \n",
       "FlexTENet (HPO)               16.792147              1.357712    1.657927   \n",
       "BART                           5.668311             11.458935    9.506232   \n",
       "TarNet (HPO)                 108.224995              0.210069    0.337117   \n",
       "DragonNet (HPO)              104.886658              9.029205   10.432764   \n",
       "RA Net (HPO)                 248.408577             26.495504   28.642944   \n",
       "SNet (HPO)                   106.030599              0.370250    0.628990   \n",
       "IPW (HPO)                   1205.171748             55.113963   55.112547   \n",
       "S Learner (HPO)             1205.113190             55.193442   55.201033   \n",
       "T Learner (HPO)             1807.565992            110.639473  110.636495   \n",
       "X Learner (HPO)             3434.412390            166.020475  165.984879   \n",
       "Forest DR Learner (HPO)     1708.428262            104.804439  104.389961   \n",
       "Forest DML (HPO)            2457.302990            115.939603  115.355165   \n",
       "DA Learner (HPO)            4820.032942            193.046603  192.999544   \n",
       "\n",
       "dataset                    RealCause Lalonde PSID                   overall  \\\n",
       "                                         ate_time    cate_time     ate_time   \n",
       "method                                                                        \n",
       "FlexTENet (no HPO)                       0.000480     0.000548     0.000481   \n",
       "IPW (no HPO)                             0.081067     0.081708     0.077551   \n",
       "S Learner (no HPO)                       0.111352     0.115120     0.115654   \n",
       "T Learner (no HPO)                       0.248036     0.245745     0.252443   \n",
       "DA Learner (no HPO)                      0.522499     0.535804     0.535191   \n",
       "CausalPFN                                2.966882     0.485863     2.534572   \n",
       "GRF (no HPO)                             3.268610     0.510705     3.219808   \n",
       "X Learner (no HPO)                       0.684558     0.691134     0.676213   \n",
       "TarNet (no HPO)                          1.713454     1.102849     1.568243   \n",
       "DragonNet (no HPO)                       3.027199     2.607162     2.587719   \n",
       "GRF (HPO)                                3.168451     3.088848     3.140710   \n",
       "XNET (no HPO)                            4.423722     3.598758     4.341404   \n",
       "RA Net (no HPO)                          4.269286     4.681562     3.828014   \n",
       "Forest DR Learner (no HPO)               4.277007     3.989793     4.148781   \n",
       "Forest DML (no HPO)                      5.377914     5.007255     5.634726   \n",
       "SNet (no HPO)                            7.347283     5.761024     7.223473   \n",
       "XNET (HPO)                               7.225780     6.218904     6.706197   \n",
       "FlexTENet (HPO)                          5.196586     7.300758     5.094625   \n",
       "BART                                    10.516291     9.264590    10.007730   \n",
       "TarNet (HPO)                            28.186210    20.035166    25.688087   \n",
       "DragonNet (HPO)                         36.160391    27.710479    32.526312   \n",
       "RA Net (HPO)                            62.936859    51.355192    56.764950   \n",
       "SNet (HPO)                             151.726262   115.182332    81.960905   \n",
       "IPW (HPO)                              335.441295   335.388256   336.480014   \n",
       "S Learner (HPO)                        335.498217   335.625539   336.523395   \n",
       "T Learner (HPO)                        358.069995   358.009164   336.598740   \n",
       "X Learner (HPO)                        801.324017   801.266232   673.518492   \n",
       "Forest DR Learner (HPO)                674.208057   673.787195   675.454211   \n",
       "Forest DML (HPO)                       679.377478   678.771401   680.568412   \n",
       "DA Learner (HPO)                      1341.972529  1341.984959  1346.093917   \n",
       "\n",
       "dataset                                  \n",
       "                              cate_time  \n",
       "method                                   \n",
       "FlexTENet (no HPO)             0.000549  \n",
       "IPW (no HPO)                   0.078507  \n",
       "S Learner (no HPO)             0.118159  \n",
       "T Learner (no HPO)             0.247548  \n",
       "DA Learner (no HPO)            0.548800  \n",
       "CausalPFN                      0.555087  \n",
       "GRF (no HPO)                   0.579403  \n",
       "X Learner (no HPO)             0.679522  \n",
       "TarNet (no HPO)                1.053019  \n",
       "DragonNet (no HPO)             1.916093  \n",
       "GRF (HPO)                      3.045631  \n",
       "XNET (no HPO)                  3.586190  \n",
       "RA Net (no HPO)                3.883391  \n",
       "Forest DR Learner (no HPO)     3.892557  \n",
       "Forest DML (no HPO)            5.222570  \n",
       "SNet (no HPO)                  5.604429  \n",
       "XNET (HPO)                     5.953733  \n",
       "FlexTENet (HPO)                7.168991  \n",
       "BART                           8.637354  \n",
       "TarNet (HPO)                  18.034589  \n",
       "DragonNet (HPO)               26.099312  \n",
       "RA Net (HPO)                  47.459009  \n",
       "SNet (HPO)                    88.576926  \n",
       "IPW (HPO)                    336.472630  \n",
       "S Learner (HPO)              336.522268  \n",
       "T Learner (HPO)              336.587934  \n",
       "X Learner (HPO)              673.465243  \n",
       "Forest DR Learner (HPO)      675.156030  \n",
       "Forest DML (HPO)             679.731633  \n",
       "DA Learner (HPO)            1346.098121  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "ate_times = defaultdict(list)\n",
    "cate_times = defaultdict(list)\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dset_result = load_all_results(dataset_name, causal_effect_path)\n",
    "    for method in methods_to_show:\n",
    "        all_rows = dset_result[method]\n",
    "        num_realizations = len(all_rows[\"pehe\"])\n",
    "        for fold_idx in range(num_realizations):\n",
    "            cate_time = float(all_rows[\"time_cate\"][fold_idx])\n",
    "            ate_time = float(all_rows[\"time_ate\"][fold_idx])\n",
    "            ate_times[method].append(ate_time)\n",
    "            cate_times[method].append(cate_time)\n",
    "med_ate_times = {}\n",
    "med_cate_times = {}\n",
    "for method in methods_to_show:\n",
    "    med_ate_times[method] = np.median([ate_times[method][i] for i in range(len(ate_times[method]))])\n",
    "    med_cate_times[method] = np.median([cate_times[method][i] for i in range(len(cate_times[method]))])\n",
    "\n",
    "# add a multicolumn to causal_effect_errors called \"overall\"\n",
    "time_spent_df[(\"overall\", \"ate_time\")] = pd.Series(med_ate_times)\n",
    "time_spent_df[(\"overall\", \"cate_time\")] = pd.Series(med_cate_times)\n",
    "# sort rows by rank\n",
    "time_spent_df = time_spent_df.sort_values(by=(\"overall\", \"cate_time\"))\n",
    "time_spent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4ff5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
